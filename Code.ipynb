{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f98470-4334-4652-862e-8be14efc1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "import nltk #text processing library for human language data.\n",
    "example = '''He is a great teacher that cares about his students. \n",
    "He is aware that we have a lot of work to do so he has always been flexible and considered with us.\n",
    "I would like to meet more teachers like him.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86624e0a-f1a5-4ba9-b3b1-1390d986135c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He is a great teacher that cares about his students.',\n",
       " 'He is aware that we have a lot of work to do so he has always been flexible and considered with us.',\n",
       " 'I would like to meet more teachers like him.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sents = sent_tokenize(example) #tokenizing text into sentences\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643a20cc-b735-4ec4-9645-f29b4d270bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c533355d-1c5b-4ce6-ae8e-1446fc6494a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'is', 'a', 'great', 'teacher', 'that', 'cares', 'about', 'his', 'students', '.']\n",
      "['He', 'is', 'aware', 'that', 'we', 'have', 'a', 'lot', 'of', 'work', 'to', 'do', 'so', 'he', 'has', 'always', 'been', 'flexible', 'and', 'considered', 'with', 'us', '.']\n",
      "['I', 'would', 'like', 'to', 'meet', 'more', 'teachers', 'like', 'him', '.']\n",
      "---------------------------------------\n",
      "['he', 'is', 'a', 'great', 'teacher', 'that', 'cares', 'about', 'his', 'students', '']\n",
      "['he', 'is', 'aware', 'that', 'we', 'have', 'a', 'lot', 'of', 'work', 'to', 'do', 'so', 'he', 'has', 'always', 'been', 'flexible', 'and', 'considered', 'with', 'us', '']\n",
      "['i', 'would', 'like', 'to', 'meet', 'more', 'teachers', 'like', 'him', '']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = [word_tokenize(sentence) for sentence in sents]\n",
    "# Print the word tokens for each sentence\n",
    "for sentence_tokens in word_tokens:\n",
    "    print(sentence_tokens)\n",
    "print(\"---------------------------------------\")\n",
    "# Function to perform lowercasing and punctuation removal\n",
    "def lowercasing_punc_removal(tokens):\n",
    "    processed_tokens = [token.lower().strip('.!?,') for token in tokens]\n",
    "    return processed_tokens\n",
    "    \n",
    "for sentence_tokens in word_tokens:\n",
    "    print(lowercasing_punc_removal(sentence_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e94170c-98dc-4c4e-b030-ea3a560ece52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#StopWords removal\n",
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b870a23a-d8c2-4eff-815b-a95f9d8f3dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'teacher', 'cares', 'students', '.']\n",
      "['aware', 'lot', 'work', 'always', 'flexible', 'considered', 'us', '.']\n",
      "['would', 'like', 'meet', 'teachers', 'like', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words from the word tokens\n",
    "filtered_word_tokens = [[word for word in sentence_tokens if word.lower() not in sw] for sentence_tokens in word_tokens]\n",
    "\n",
    "# Print the filtered word tokens for each sentence\n",
    "for sentence_tokens in filtered_word_tokens:\n",
    "    print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c327fc-7693-4ec4-844f-583a095645cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['great', 'teacher', 'care', 'student', '.'],\n",
       " ['awar', 'lot', 'work', 'alway', 'flexibl', 'consid', 'us', '.'],\n",
       " ['would', 'like', 'meet', 'teacher', 'like', '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_word_tokens = [\n",
    "    [stemmer.stem(word.lower()) for word in sentence_tokens if word.lower() not in sw]\n",
    "    for sentence_tokens in word_tokens\n",
    "]\n",
    "stemmed_word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4efbcf1d-27ec-4918-b1bf-932abe1fdb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great teacher care student .', 'awar lot work alway flexibl consid us .', 'would like meet teacher like .']\n"
     ]
    }
   ],
   "source": [
    "#let's convert text into bag of words\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert the list of lists to a list of sentences\n",
    "sentences = [' '.join(word_list) for word_list in stemmed_word_tokens]\n",
    "# Print the resulting list of sentences\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21582f7b-8d56-4699-be8b-0f9798946bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1 0 0 0 1 1 0 0 0]\n",
      " [1 1 0 1 1 0 0 1 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 2 0 1 0 1 0 0 1]]\n",
      "{'great': 5, 'teacher': 10, 'care': 2, 'student': 9, 'awar': 1, 'lot': 7, 'work': 12, 'alway': 0, 'flexibl': 4, 'consid': 3, 'us': 11, 'would': 13, 'like': 6, 'meet': 8}\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "#let's convert text into bag of words\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Initializing CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "# Convert the processed text (stemmed corpus) into numerical features\n",
    "vc = cv.fit_transform(sentences) # fit means creating a fixed vocabulary\n",
    "# transform means converting text into numerical vector\n",
    "\n",
    "print(vc.toarray()) # printing numerical matrix\n",
    "print(cv.vocabulary_) # printing the vocabulary (dictionary of words and their corresponding indices)\n",
    "# In our BoW, the word 'great' is placed at index 5 and it occurred only once in our example.\n",
    "\n",
    "print(len(cv.vocabulary_)) # In our BoW, we have 14 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a390473-ff9b-47f2-bf6d-6b27c3741c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: great teacher care student .\n",
      "Sentiment Scores: {'neg': 0.0, 'neu': 0.215, 'pos': 0.785, 'compound': 0.8074}\n",
      "Sentiment Label: 1\n",
      "---------------------\n",
      "Sentence: awar lot work alway flexibl consid us .\n",
      "Sentiment Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Sentiment Label: 0\n",
      "---------------------\n",
      "Sentence: would like meet teacher like .\n",
      "Sentiment Scores: {'neg': 0.0, 'neu': 0.375, 'pos': 0.625, 'compound': 0.6124}\n",
      "Sentiment Label: 1\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Arailym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Now let's make sentiment analysis with a help of lexicon based approach Vader dictionary\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Perform sentiment analysis on each sentence\n",
    "for sentence in sentences:\n",
    "    sentiment_scores = analyzer.polarity_scores(sentence)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "\n",
    "    # Determine sentiment label based on the compound score\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment_label = \"1\"\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment_label = \"-1\"\n",
    "    else:\n",
    "        sentiment_label = \"0\"\n",
    "\n",
    "    # Print the sentence, sentiment scores, and sentiment label\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Sentiment Scores:\", sentiment_scores)\n",
    "    print(\"Sentiment Label:\", sentiment_label)\n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed5fbb0a-157f-4b5a-9193-a53e9629b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Answers  Classification\n",
      "0                                            Nothing             NaN\n",
      "1                    Maybe less complicated the test             NaN\n",
      "2                                            Nothing             NaN\n",
      "3  In order to make it easier to follow the cours...             NaN\n",
      "4  I really enjoyed the theoretical part of the l...             NaN\n",
      "5  It would be interesting to meet stakeholders w...             NaN\n",
      "6  I have no suggestions, I think this was the be...             NaN\n",
      "7  She is a very nice teacher that wants her stud...             NaN\n",
      "8  He is a great teacher that cares about his stu...             NaN\n",
      "9  He is a very nice teacher. He made the subject...             NaN\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis with a help of Vader dictionary\n",
    "import pandas as pd # pandas library -> efficiently store and manipulate CSV files.\n",
    "dataset = pd.read_csv('C:/Users/Arailym/Downloads/exchange_students_feedback.csv')\n",
    "print(dataset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61f33910-c9ad-4325-801c-515b293b713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Arailym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER predicted sentiments saved to 'vader_dictionary.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis with a help of Vader dictionary\n",
    "import pandas as pd # pandas library -> efficiently store and manipulate CSV files.\n",
    "dataset = pd.read_csv('C:/Users/Arailym/Downloads/exchange_students_feedback.csv')\n",
    "print(dataset.head(10))\n",
    "import string\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Preprocessing function \n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming using Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "dataset = pd.read_csv('C:/Users/Arailym/Downloads/exchange_students_feedback.csv')\n",
    "dataset['preprocessed_text'] = dataset['Answers'].apply(preprocess_text)\n",
    "\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment label based on the compound score\n",
    "def get_sentiment_label(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "        \n",
    "# Analyze the sentiment of each preprocessed text using VADER\n",
    "dataset['vader_sentiment_score'] = dataset['preprocessed_text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "dataset['vader_sentiment_label'] = dataset['vader_sentiment_score'].apply(get_sentiment_label)\n",
    "\n",
    "# Save the results into a new DataFrame\n",
    "result_data = dataset[['Answers', 'vader_sentiment_label', 'vader_sentiment_score']]\n",
    "\n",
    "# Convert the new DataFrame into a CSV file\n",
    "result_data.to_csv('vader_dictionary.csv', index=False)\n",
    "print(\"VADER predicted sentiments saved to 'vader_dictionary.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e971dcc2-4084-40f6-9d01-5c0e993d9586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Arailym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER predicted sentiments saved to 'vader_dictionary.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis with a help of Vader dictionary\n",
    "import pandas as pd # pandas library -> efficiently store and manipulate CSV files.\n",
    "dataset = pd.read_csv('C:/Users/Arailym/Downloads/exchange_students_feedback.csv')\n",
    "print(dataset.head(10))\n",
    "import string\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Preprocessing function \n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming using Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "dataset = pd.read_csv('C:/Users/Arailym/Downloads/exchange_students_feedback.csv')\n",
    "dataset['preprocessed_text'] = dataset['Answers'].apply(preprocess_text)\n",
    "\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment label based on the compound score\n",
    "def get_sentiment_label(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "        \n",
    "# Analyze the sentiment of each preprocessed text using VADER\n",
    "dataset['vader_sentiment_score'] = dataset['preprocessed_text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "dataset['vader_sentiment_label'] = dataset['vader_sentiment_score'].apply(get_sentiment_label)\n",
    "\n",
    "# Save the results into a new DataFrame\n",
    "result_data = dataset[['Answers', 'vader_sentiment_label', 'vader_sentiment_score']]\n",
    "\n",
    "# Convert the new DataFrame into a CSV file\n",
    "result_data.to_csv('vader_dictionary.csv', index=False)\n",
    "print(\"VADER predicted sentiments saved to 'vader_dictionary.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "137cf46f-4a29-4fae-8911-c2ab13e5013e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Classification                                            Answers\n",
      "0               0      Longer exams, the were not a lot of questions\n",
      "1               1  Business communication was my favorite subject...\n",
      "2              -1  Boring class, maybe du to the fact that it was...\n",
      "3               0                                     no suggestions\n",
      "4               1                                    everything good\n",
      "5              -1                                      less homework\n",
      "6              -1   Focus it either for piloting or for engineering.\n",
      "7               0                                                ---\n",
      "8               0                                     no suggestions\n",
      "9              -1                                     more exercises\n",
      "Index(['Classification', 'Answers'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis with a help of Naive Bayes algorithm\n",
    "import pandas as pd # pandas library -> efficiently store and manipulate CSV files.\n",
    "test_data = pd.read_csv('C:/Users/Arailym/Documents/Research paper/testing_data.csv')\n",
    "train_data = pd.read_csv('C:/Users/Arailym/Documents/Research paper/train_data.csv')\n",
    "\n",
    "print(train_data.head(10))\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06a4909b-d036-46f9-a1b0-dce1088b4407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the 'Answers' column in the DataFrame\n",
    "train_data['preprocessed_text'] = train_data['Answers'].apply(preprocess_text)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# Fit and transform the preprocessed text to create the feature matrix\n",
    "X_train_features = vectorizer.fit_transform(train_data['preprocessed_text'])\n",
    "print(\"Feature Matrix:\")\n",
    "print(X_train_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dbb54e1c-cad1-4cc6-affa-6bb46dc8b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classifier trained successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Initialize the Naive Bayes classifier (Multinomial Naive Bayes)\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier using the feature matrix and sentiment labels\n",
    "nb_classifier.fit(X_train_features, train_data['Classification'])\n",
    "\n",
    "# Print a message to indicate that the training is complete\n",
    "print(\"Naive Bayes classifier trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "075c187e-c9f0-4fcd-8909-2130b00cf536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Classification                                            Answers  \\\n",
      "0               0                                            nothing   \n",
      "1               1  The teacher listened to Erasmus students and w...   \n",
      "2               0  the only probelm sometimes was the connection ...   \n",
      "3              -1  I would have appreciate the teacher to do powe...   \n",
      "4              -1            it was hard to contact with the teacher   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0                                               noth  \n",
      "1  teacher listen erasmu student welcom teacher i...  \n",
      "2  probelm sometim connect internet 's someth som...  \n",
      "3  would appreci teacher powerpoint present lectu...  \n",
      "4                               hard contact teacher  \n",
      "Index(['Classification', 'Answers', 'preprocessed_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the 'Answer' column in the DataFrame\n",
    "test_data['preprocessed_text'] = test_data['Answers'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the preprocessed testing data\n",
    "print(test_data.head())\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8bf81d7c-7c12-4ec7-987b-ad146ab2ab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix for Testing Data:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Transform the preprocessed testing data using the same vectorizer\n",
    "X_test_data_features = vectorizer.transform(test_data['preprocessed_text'])\n",
    "\n",
    "# Display the feature matrix (sparse matrix representation) of the testing data\n",
    "print(\"Feature Matrix for Testing Data:\")\n",
    "print(X_test_data_features.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ca7695c-dc4b-43b4-924c-24adf043e055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiments saved to 'naive_bayes_sentiment_prediction.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Use the trained classifier to make predictions on the testing data\n",
    "predictions = nb_classifier.predict(X_test_data_features)\n",
    "\n",
    "# Combine the predicted sentiment labels with the original testing data\n",
    "test_data['predicted_sentiment'] = predictions\n",
    "\n",
    "# Save the combined data into a new DataFrame\n",
    "result_data = test_data[['Answers', 'predicted_sentiment']]\n",
    "\n",
    "# Convert the new DataFrame into a CSV file\n",
    "result_data.to_csv('naibe_bayes_rez.csv', index=False)\n",
    "\n",
    "print(\"Predicted sentiments saved to 'naive_bayes_sentiment_prediction.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a09ca58-75a1-4d32-8ec9-dfdfa4c2fbdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [224, 44]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m predicted_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate precision and recall\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [224, 44]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the ground truth labels for the testing data\n",
    "test_data = pd.read_csv('C:/Users/Arailym/Downloads/labeled_feedback.csv')\n",
    "actual_labels = test_data['Classification']\n",
    "\n",
    "# Load the predicted labels from the CSV file generated by the Naive Bayes classifier\n",
    "predicted_data = pd.read_csv('C:/Users/Arailym/naibe_bayes_rez.csv')\n",
    "predicted_labels = predicted_data['predicted_sentiment']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(actual_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(actual_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1312ce-454c-4eb4-aeae-c3797c035927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eccde53-3506-47d4-811c-5ab05f03724d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Arailym/Downloads/labeled_feedback.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to the 'Answers' column in the training DataFrame\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswers\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mpreprocess_text\u001b[49m)\n\u001b[0;32m     14\u001b[0m preprocessed_example \u001b[38;5;241m=\u001b[39m preprocess_text(example) \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Creating a DataFrame with the preprocessed example text ensures \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# that the input data format is compatible with the trained Naive Bayes classifier\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_text' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "example = '''He is a great teacher that cares about his students. \n",
    "He is aware that we have a lot of work to do so he has always been flexible and considered with us. \n",
    "I would like to meet more teachers like him.'''\n",
    "\n",
    "train_data = pd.read_csv('C:/Users/Arailym/Downloads/labeled_feedback.csv')\n",
    "# Apply preprocessing to the 'Answers' column in the training DataFrame\n",
    "train_data['preprocessed_text'] = train_data['Answers'].apply(preprocess_text)\n",
    "\n",
    "preprocessed_example = preprocess_text(example) \n",
    "# Creating a DataFrame with the preprocessed example text ensures \n",
    "# that the input data format is compatible with the trained Naive Bayes classifier\n",
    "example_data = pd.DataFrame({'Answers': [preprocessed_example]})\n",
    "\n",
    "# Initialize the CountVectorizer and fit_transform the preprocessed text\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_features = vectorizer.fit_transform(train_data['preprocessed_text'])\n",
    "\n",
    "# Initialize the Naive Bayes classifier (Multinomial Naive Bayes)\n",
    "nb_classifier = MultinomialNB()\n",
    "# Train the classifier using the feature matrix and sentiment labels\n",
    "nb_classifier.fit(X_train_features, train_data['Classification'])\n",
    "# Transform the example data using the same vectorizer\n",
    "X_example_features = vectorizer.transform(example_data['Answers'])\n",
    "\n",
    "# Use the trained classifier to make predictions on the example data\n",
    "example_prediction_prob = nb_classifier.predict_proba(X_example_features)\n",
    "predicted_class_index = example_prediction_prob.argmax()  # Get the index of the predicted class\n",
    "predicted_sentiment_label = nb_classifier.classes_[predicted_class_index]\n",
    "\n",
    "# Get the confidence (probability) of the prediction\n",
    "confidence = example_prediction_prob[0][predicted_class_index]\n",
    "\n",
    "# Print the predicted sentiment and confidence for the example text\n",
    "print(\"Predicted Sentiment for the text is :\", predicted_sentiment_label)\n",
    "print(\"Confidence:\", confidence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803b523-8902-47f2-b6ea-76642a09f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9a7edc-722a-48da-b3cc-632bd3988020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Arailym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Arailym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Arailym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming using Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Load the testing data from CSV\n",
    "test_data = pd.read_csv('C:/Users/Arailym/Downloads/exchange_students_feedback.csv')\n",
    "# Load the labeled data from CSV\n",
    "labeled_data = pd.read_csv('C:/Users/Arailym/Downloads/labeled_feedback.csv')\n",
    "\n",
    "# Preprocess the text in the testing data\n",
    "test_data['preprocessed_text'] = test_data['Answers'].apply(preprocess_text)\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bcdfdc5-f6c2-40ea-b6e4-1b1c9f91c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get sentiment label based on the compound score and threshold\n",
    "def get_sentiment_label(compound_score, threshold):\n",
    "    if compound_score >= threshold:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -threshold:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a235a7-25f9-4071-9a68-f6c8d1834570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test thresholds from 0.01 to 0.1\n",
    "thresholds = [i/100 for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb029ba-eb16-4632-b1c4-3c84ca0121ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1758bb3-fb9a-42cd-bca7-9156a7e17677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b14cb5-ac5e-4d9e-8156-efacd84ace39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8839285714285714\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the ground truth labels for the testing data\n",
    "test_data = pd.read_csv('C:/Users/Arailym/Downloads/labeled_feedback.csv')\n",
    "actual_labels = test_data['Classification']\n",
    "\n",
    "# Load the predicted labels from the CSV file generated by the Naive Bayes classifier\n",
    "predicted_data = pd.read_csv('C:/Users/Arailym/nb_rez.csv')\n",
    "predicted_labels = predicted_data['predicted_sentiment']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f8c30e1-524a-4a78-8fcd-05157d8f4e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with numerical sentiment labels saved to 'labeled_feedback_numeric.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the labeled data into a DataFrame (assuming it is already available)\n",
    "monkeylearn = pd.read_csv('C:/Users/Arailym/Downloads/processed_batch.csv')\n",
    "\n",
    "# Mapping dictionary for sentiment labels\n",
    "sentiment_map = {'Positive': 1, 'Negative': -1, 'Neutral': 0}\n",
    "\n",
    "# Convert the 'Classification' column to numerical values using the mapping\n",
    "monkeylearn['sentiment_numeric'] = monkeylearn['Classification'].map(sentiment_map)\n",
    "\n",
    "# Save the DataFrame with the converted sentiment labels to a CSV file\n",
    "monkeylearn.to_csv('monkey.csv', index=False)\n",
    "\n",
    "print(\"DataFrame with numerical sentiment labels saved to 'labeled_feedback_numeric.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f158ec4-8e39-4507-93c3-9651175cf627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a137f2a2-db0c-4e88-b9dc-f766741c8977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Answers  Classification\n",
      "0                                            Nothing             NaN\n",
      "1                    Maybe less complicated the test             NaN\n",
      "2                                            Nothing             NaN\n",
      "3  In order to make it easier to follow the cours...             NaN\n",
      "4  I really enjoyed the theoretical part of the l...             NaN\n",
      "5  It would be interesting to meet stakeholders w...             NaN\n",
      "6  I have no suggestions, I think this was the be...             NaN\n",
      "7  She is a very nice teacher that wants her stud...             NaN\n",
      "8  He is a great teacher that cares about his stu...             NaN\n",
      "9  He is a very nice teacher. He made the subject...             NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Arailym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER predicted sentiments saved to 'one_treshold.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis with a help of Vader dictionary\n",
    "import pandas as pd # pandas library -> efficiently store and manipulate CSV files.\n",
    "dataset = pd.read_csv('C:/Users/Arailym/Downloads/exchange_students_feedback.csv')\n",
    "print(dataset.head(10))\n",
    "import string\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Preprocessing function \n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming using Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "dataset = pd.read_csv('C:/Users/Arailym/Downloads/exchange_students_feedback.csv')\n",
    "dataset['preprocessed_text'] = dataset['Answers'].apply(preprocess_text)\n",
    "\n",
    "# Initialize the SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment label based on the compound score\n",
    "def get_sentiment_label(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return '1'\n",
    "    elif compound_score <= -0.05:\n",
    "        return '-1'\n",
    "    else:\n",
    "        return '0'\n",
    "        \n",
    "# Analyze the sentiment of each preprocessed text using VADER\n",
    "dataset['vader_sentiment_score'] = dataset['preprocessed_text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "dataset['vader_sentiment_label'] = dataset['vader_sentiment_score'].apply(get_sentiment_label)\n",
    "\n",
    "# Save the results into a new DataFrame\n",
    "result_data = dataset[['Answers', 'vader_sentiment_label', 'vader_sentiment_score']]\n",
    "\n",
    "# Convert the new DataFrame into a CSV file\n",
    "result_data.to_csv('zero_treshold.csv', index=False)\n",
    "print(\"VADER predicted sentiments saved to 'one_treshold.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af06c0-fc31-4279-967b-a842f154049a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d245787-eb39-44d1-81aa-d8926b75c28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d8b6e1-5b96-4c3b-9f2e-260abc219a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6160714285714286\n",
      "Precision: 0.737417924829275\n",
      "Recall: 0.6160714285714286\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Load the ground truth labels for the testing data\n",
    "test_data = pd.read_csv('C:/Users/Arailym/Downloads/labeled_feedback.csv')\n",
    "actual_labels = test_data['Classification']\n",
    "\n",
    "# Load the predicted labels from the CSV file generated by the Naive Bayes classifier\n",
    "predicted_data = pd.read_csv('C:/Users/Arailym/one_treshold.csv')\n",
    "predicted_labels = predicted_data['vader_sentiment_label']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(actual_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(actual_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "623df8f0-d7c5-4245-b8ab-56b3216c2983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6160714285714286\n",
      "Precision: 0.737417924829275\n",
      "Recall: 0.6160714285714286\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Load the ground truth labels for the testing data\n",
    "test_data = pd.read_csv('C:/Users/Arailym/Downloads/labeled_feedback.csv')\n",
    "actual_labels = test_data['Classification']\n",
    "\n",
    "# Load the predicted labels from the CSV file generated by the Naive Bayes classifier\n",
    "predicted_data = pd.read_csv('C:/Users/Arailym/two_treshold.csv')\n",
    "predicted_labels = predicted_data['vader_sentiment_label']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(actual_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(actual_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47e010f3-efef-45cb-bd12-3e37d8a2fce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6116071428571429\n",
      "Precision: 0.7350653065460239\n",
      "Recall: 0.6116071428571429\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Load the ground truth labels for the testing data\n",
    "test_data = pd.read_csv('C:/Users/Arailym/Downloads/labeled_feedback.csv')\n",
    "actual_labels = test_data['Classification']\n",
    "\n",
    "# Load the predicted labels from the CSV file generated by the Naive Bayes classifier\n",
    "predicted_data = pd.read_csv('C:/Users/Arailym/three_treshold.csv')\n",
    "predicted_labels = predicted_data['vader_sentiment_label']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(actual_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(actual_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f3993da-c2be-4d67-9805-deea40178ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6116071428571429\n",
      "Precision: 0.7350653065460239\n",
      "Recall: 0.6116071428571429\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Load the ground truth labels for the testing data\n",
    "test_data = pd.read_csv('C:/Users/Arailym/Downloads/labeled_feedback.csv')\n",
    "actual_labels = test_data['Classification']\n",
    "\n",
    "# Load the predicted labels from the CSV file generated by the Naive Bayes classifier\n",
    "predicted_data = pd.read_csv('C:/Users/Arailym/five_treshold.csv')\n",
    "predicted_labels = predicted_data['vader_sentiment_label']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(actual_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(actual_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aee93f0-d4b4-4f35-9233-18fcf3234e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6383928571428571\n",
      "Precision: 0.6526430320111916\n",
      "Recall: 0.6383928571428571\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Load the ground truth labels for the testing data\n",
    "test_data = pd.read_csv('C:/Users/Arailym/Downloads/labeled_feedback.csv')\n",
    "actual_labels = test_data['Classification']\n",
    "\n",
    "# Load the predicted labels from the CSV file generated by the Naive Bayes classifier\n",
    "predicted_data = pd.read_csv('C:/Users/Arailym/monkey.csv')\n",
    "predicted_labels = predicted_data['sentiment_numeric']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(actual_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(actual_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cf2ef09-4111-44e0-a241-b43e015df6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7272727272727273\n",
      "Precision: 0.738755980861244\n",
      "Recall: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Load the ground truth labels for the testing data\n",
    "test_data = pd.read_csv('C:/Users/Arailym/naibe_bayes_rez.csv')\n",
    "actual_labels = test_data['labeled_sentiment']\n",
    "\n",
    "predicted_labels = test_data['predicted_sentiment']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(actual_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(actual_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343fb731-1134-4341-93cb-6c76eea9b010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
